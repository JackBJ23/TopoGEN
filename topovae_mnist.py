# -*- coding: utf-8 -*-
"""topo_vae_models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cuKKicFq5l-lhM0RjqHaNuDwPlqodF5-
"""

# ripser_parallel: https://giotto-ai.github.io/giotto-ph/build/html/modules/ripser_parallel.html
# bottleneck dist: https://persim.scikit-tda.org/en/latest/notebooks/distances.html

!pip3 install giotto-ph
import numpy as np
from gph import ripser_parallel

!pip install ipython

from IPython.display import Image  # to display images
import sys
!{sys.executable} -m pip install giotto-tda

# here comes our protagonist!
from gph import ripser_parallel

# Import utils
import numpy as np
from gtda.homology._utils import _postprocess_diagrams

# To generate dataset
from sklearn import datasets

# Plotting
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from plotly import graph_objects as go
from gtda.plotting import plot_diagram, plot_point_cloud

!pip3 install Cython ripser tadasets
!pip3 install persim #provides matching
#used this: https://persim.scikit-tda.org/en/latest/notebooks/distances.html
import persim
import tadasets
import ripser
#other packages:
import math
import scipy
import torch
import random

!pip3 install torchvision
!pip3 install tqdm
!pip3 install six

#%matplotlib inline
import argparse
import os
import random
import torch
import torchvision
import torch.nn as nn
import torch.nn.parallel
import torch.optim as optim
import torch.utils.data
from torch.utils.data import DataLoader
from torchvision.utils import make_grid

import torchvision.datasets as dset
from torchvision import datasets

import torchvision.transforms as transforms
import torchvision.utils as vutils
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from IPython.display import HTML

#for dataset:
!pip install Pillow
!pip install PIL
!pip install image
import PIL.Image as Image
import zipfile

device = "cuda" if torch.cuda.is_available() else "cpu"

"""Topological regularizers:"""

## push0 and push1 are not used here

def get_dgm(point_cloud, deg):
  # Compute the persistence diagram without backprop
  with torch.no_grad():
        ##convert points for computing PD:
        points_np = point_cloud.numpy()
        # get PD with generators:
        dgm = ripser_parallel(points_np, maxdim=deg, return_generators=True)
  return dgm

#euclidean dist for torch tensors:
def dist(point1, point2):
    return torch.sqrt(torch.sum((point2 - point1)**2))

#euclidean dist for numpy points:
def dist_np(point1, point2):
    return np.sqrt(np.sum((point2 - point1)**2))

#supremum dist for torch tensors:
def dist_sup_tc(b1, d1, b2, d2):
    # Calculate the sup norm between points (b1, d1) and (b2, d2)
    return torch.max(torch.abs(b1 - b2), torch.abs(d1 - d2))

def d_bottleneck0(point_cloud, dgm, dgm2): # got_loss=1 if got loss, =0 if loss does not depend on dgm
    got_loss = 1
    with torch.no_grad():
        distance_bottleneck, matching = persim.bottleneck(dgm['dgms'][0][:-1], dgm2['dgms'][0][:-1], matching=True)
        #find the pair that gives the max distance:
        index = np.argmax(matching[:, 2])
        i, j = int(matching[index][0]), int(matching[index][1]) #i, j: the i-th and j-th point of the dgm1, dgm2 respectively, that give the bottleneck dist.
        # (if the largest dist is point<->diagonal: i or j is -1)
        #i is the i-th pt in dgm and j is the j-th pt in dgm2 which give the bottleneck dist (i.e. it is the largest dim)
        #for the loss, need to know what is the point i (learnable), i=(distmatrix[xi,yi],distmatrix[ai,bi]) in the distance matrix for some 4 indices
        #but gen[0]
        # i is the index of a point of the PD. but (gens[i][1], gens[i][2]) is the pair of vertices of the point cloud that correspond to the point i=(0,d), with d=dist(gens[i][1]-gens[i][2])
        #get the 2 points that give the distance of the i-th pt in dgm in the 1st diagram and compute the loss:
    if i>=0:
      point1_dgm1 = point_cloud[dgm['gens'][0][i][1]]
      point2_dgm1 = point_cloud[dgm['gens'][0][i][2]]

    if i>=0 and j>=0:
      loss = torch.abs(dist(point1_dgm1, point2_dgm1) - dgm2['dgms'][0][j][1])
    else:
      if i==-1: #so the j-th point from dgm2 is matched to the diagonal -> backprop through loss would give 0 -> goal: make points further from diag
        #new_bdist = torch.abs(dist(point1_dgm2, point2_dgm2) - 0.)/2
        loss = 0
        got_loss = 0
      else: #then  j==-1, so the i-th point from dgm1 is matched to the diagonal
        loss = dist(point1_dgm1, point2_dgm1)/2.

    return loss, got_loss

def d_bottleneck1(point_cloud, dgm, dgm2): # got_loss=1 if got loss, =0 if loss does not depend on dgm
    got_loss = 1
    if len(dgm['dgms'][1])==0: return 0, 0
    if len(dgm2['dgms'][1])==0: dgm2['dgms'][1] = [[0.,0.]] #print("error2")
    with torch.no_grad():
        distance_bottleneck, matching = persim.bottleneck(dgm['dgms'][1], dgm2['dgms'][1], matching=True)
        #find the pair that gives the max distance:
        index = np.argmax(matching[:, 2])
        i, j = int(matching[index][0]), int(matching[index][1])
        #i is the i-th pt in dgm and j is the j-th pt in dgm2 which give the bottleneck dist (i.e. it is the largest dim)
        #for the loss, need to know what is the point i (learnable), i=(distmatrix[xi,yi],distmatrix[ai,bi]) in the distance matrix for some 4 indices
        # i is the index of a point of the PD. but (gens[i][1], gens[i][2]) is the pair of vertices of the point cloud that correspond to the point i=(0,d), with d=dist(gens[i][1]-gens[i][2])

    #get the 2 points that give the distance of the i-th pt in dgm in the 1st diagram:
    #if i>0, then the pt of dgm1 is off-diag:
    if i>=0:
      point0_dgm1 = point_cloud[dgm['gens'][1][0][i][0]]
      point1_dgm1 = point_cloud[dgm['gens'][1][0][i][1]]
      point2_dgm1 = point_cloud[dgm['gens'][1][0][i][2]]
      point3_dgm1 = point_cloud[dgm['gens'][1][0][i][3]]
      birth_dgm1 = dist(point0_dgm1, point1_dgm1)
      death_dgm1 = dist(point2_dgm1, point3_dgm1)
    #get the 2 points that give the distance of the j-th pt in dgm in the 2nd diagram:
    if j>=0:
      birth_dgm2 = dgm2['dgms'][1][j][0]
      death_dgm2 = dgm2['dgms'][1][j][1]

    if i>=0 and j>=0:
      loss = dist_sup_tc(birth_dgm1, death_dgm1, birth_dgm2, death_dgm2)
    else:
      if i==-1: #so the j-th point from dgm2 is matched to the diagonal
        loss = 0
        got_loss = 0
      else: #then j==-1, so the i-th point from dgm1 is matched to the diagonal
        loss = (death_dgm1 - birth_dgm1)/2.

    return loss, got_loss

def dist_2(a, b, c, d):
    return (a - c)**2 + (b - d)**2

#return Reininghaus kernel ksigma: (could make it slightly faster with different functions for each dgm (dgm2 does not need backpropagation)), but let it same for all dgms
def ksigma0(point_cloud, point_cloud2, dgm, dgm2): #maxdim of both dgms: 0
    sigma = 0.01
    ksigma = 0
    ## use formula for k_sigma from paper (https://arxiv.org/pdf/1412.6821.pdf):
    for i in range(len(dgm['gens'][0])):
        # pt in dgm: (0,d), d=dist(p1,p2)
        p1, p2 = point_cloud[dgm['gens'][0][i][1]], point_cloud[dgm['gens'][0][i][2]]
        d1 = dist(p1, p2)
        for j in range(len(dgm2['gens'][0])):
           #pt in dgm2: (0,d), d=dist(q1,q2)
           q1, q2 = point_cloud2[dgm2['gens'][0][j][1]], point_cloud2[dgm2['gens'][0][j][2]]
           d2 = dist(q1, q2)
           ksigma += torch.exp(-dist_2(0, d1, 0, d2)/(8*sigma)) - torch.exp(-dist_2(0, d1, d2, 0)/(8*sigma))

    ksigma *= 1/(8*3.141592*sigma)
    return ksigma

#return pseudo-distance that comes from ksigma and squared, dsigma**2:
def dsigma0(point_cloud, point_cloud2, dgm, dgm2):
    k11 = ksigma0(point_cloud, point_cloud, dgm, dgm)
    #k22 = ksigma(point_cloud2, point_cloud2)
    k12 = ksigma0(point_cloud, point_cloud2, dgm, dgm2)
    #return k11 + k22 - 2*k12
    return k11 - 2*k12 #no need of k22 since no backpropagation through it (fixed point cloud)

#the only diff from ksigma (deg0) is how to take the pt of dgms (b,d) wrt the pts of the point clouds, for the backpropagation
def ksigma1(point_cloud, point_cloud2, dgm, dgm2): #maxdim of both dgms: 1
    sigma = 0.01
    if len(dgm['dgms'][1])==0 or len(dgm['dgms'][1])==0: return 0, 0

    ksigma1 = 0
    ## use formula for k_sigma from paper (https://arxiv.org/pdf/1412.6821.pdf):
    for i in range(len(dgm['gens'][1])):
        # pt in dgm: (b1,d1), with b1, d1 = dist(p2, p1), dist(dist(p3, p4)
        p1, p2, p3, p4 = point_cloud[dgm['gens'][1][0][i][0]], point_cloud[dgm['gens'][1][0][i][1]], point_cloud[dgm['gens'][1][0][i][2]], point_cloud[dgm['gens'][1][0][i][3]]
        b1 = dist(p1,p2)
        d1 = dist(p3,p4)

        for j in range(len(dgm2['gens'][1])):
          #pt in dgm2: (b2,d2)
          q1, q2, q3, q4 = point_cloud2[dgm2['gens'][1][0][j][0]], point_cloud2[dgm2['gens'][1][0][j][1]], point_cloud2[dgm2['gens'][1][0][j][2]], point_cloud2[dgm2['gens'][1][0][j][3]]
          b2 = dist(q1,q2)
          d2 = dist(q3,q4)

          ksigma1 += torch.exp(-dist_2(b1, d1, b2, d2)/(8*sigma)) - torch.exp(-dist_2(b1, d1, d2, b2)/(8*sigma))

    ksigma1 *= 1/(8*3.141592*sigma)
    return ksigma1, 1

def dsigma1(point_cloud, point_cloud2, dgm, dgm2):
    k12, gotloss = ksigma1(point_cloud, point_cloud2, dgm, dgm2)
    if gotloss == 0: return 0, 0
    # if both dgm and dgm2 have at least 1 point:
    k11, gotloss = ksigma1(point_cloud, point_cloud, dgm, dgm)
    return k11 + (-2.) * k12, 1

def density(point_cloud, dgm, sigma, scale, x):
  tot = 0
  density_x = 0 #density at x
  for i in range(len(dgm['dgms'][0])-1):
    p1, p2 = point_cloud[dgm['gens'][0][i][1]], point_cloud[dgm['gens'][0][i][2]] #pt (0,d) with d=dist(p1,p2) (euclidean dist)
    d = dist(p1, p2) #pt of pt cloud is (0,d)
    density_x += d**3 * torch.exp(-((d-x)/sigma)**2)

  return density_x * scale

def loss_density(point_cloud, point_cloud2, dgm, dgm2, sigma, scale, maxrange, npoints, plot): #dgm of deg0
  xs = np.linspace(0., maxrange, npoints)
  loss = 0
  ## compute difference between both functions in 100 pts (those given by xs)
  dens=[]
  dens0=[]
  for x in xs:
    dx = density(point_cloud, dgm, sigma, scale, x)
    d0x = density(point_cloud2, dgm2, sigma, scale, x)
    loss += (dx - d0x)**2
    if plot:
      with torch.no_grad():
        dens.append(dx.detach().numpy())
        dens0.append(d0x.detach().numpy())

  if plot: #plot both density functions (blue: the one of dgm being learned)
    plt.plot(xs, dens, color='blue')
    plt.plot(xs, dens0, color='red')
    plt.show()

  return loss

## use: sigma=0.2, scale=0.002, maxrange=35., npoints=100

def loss_persentropy0(point_cloud, dgm, dgm2): #dgm of deg0
  L = 0
  for i in range(len(dgm['dgms'][0])-1): L += dist(point_cloud[dgm['gens'][0][i][1]], point_cloud[dgm['gens'][0][i][2]])
  pers = 0
  for i in range(len(dgm['dgms'][0])-1):
    p1, p2 = point_cloud[dgm['gens'][0][i][1]], point_cloud[dgm['gens'][0][i][2]] #pt (0,d) with d=dist(p1,p2) (euclidean dist)
    d = dist(p1, p2) #pt of pt cloud is (0,d)
    pers += d * torch.log(d/L)

  ##get pers entropy of dgm2:
  L2 = 0
  pers2 = 0
  for i in range(len(dgm2['dgms'][0])-1): L2 += dgm2['dgms'][0][i][1]
  for i in range(len(dgm2['dgms'][0])-1): pers2 += dgm2['dgms'][0][i][1] * math.log(dgm2['dgms'][0][i][1] / L2)

  return (pers/L - pers2/L2)**2

def loss_persentropy1(point_cloud, dgm, dgm2): #dgm of deg1. returns loss, got_loss (0 if did not get it)
  if len(dgm['dgms'][1])==0 or len(dgm2['dgms'][1])==0: return 0, 0 #no loss if dgm has no off-diag points

  #entropy of dgm:
  L = 0
  for i in range(len(dgm['dgms'][1])):
        # pt in dgm: (b1,d1), with b1, d1 = dist(p2, p1), dist(dist(p3, p4)
        p1, p2, p3, p4 = point_cloud[dgm['gens'][1][0][i][0]], point_cloud[dgm['gens'][1][0][i][1]], point_cloud[dgm['gens'][1][0][i][2]], point_cloud[dgm['gens'][1][0][i][3]]
        b1 = dist(p1,p2)
        d1 = dist(p3,p4)
        L += d1 - b1
  pers = 0
  for i in range(len(dgm['gens'][1])):
        # pt in dgm: (b1,d1), with b1, d1 = dist(p2, p1), dist(dist(p3, p4)
        p1, p2, p3, p4 = point_cloud[dgm['gens'][1][0][i][0]], point_cloud[dgm['gens'][1][0][i][1]], point_cloud[dgm['gens'][1][0][i][2]], point_cloud[dgm['gens'][1][0][i][3]]
        b1 = dist(p1,p2)
        d1 = dist(p3,p4)
        pers += (d1-b1) * torch.log((d1-b1)/L)

  #entropy of dgm2:
  L2 = 0
  for i in range(len(dgm2['dgms'][1])):
        L2 += dgm2['dgms'][1][i][1] - dgm2['dgms'][1][i][0]
  pers2 = 0
  for i in range(len(dgm2['gens'][1])):
        lifespan = dgm2['dgms'][1][i][1] - dgm2['dgms'][1][i][0]
        pers += lifespan * torch.log(lifespan/L)
  if len(dgm2['dgms'][1])==0: L2 = 1

  return (pers/L - pers2/L2)**2, 1

"""Experiments with VAE and MNIST:
0. No topoloss2
1. loss_topo0
2. loss_topo0 + loss_topo1
3. loss_persentropy0 + loss_persentropy1
4. loss_persentropy0 + dsigma1
5. density0 + dsigma1
6. dsigma0 + dsigma1

Code of VAE and FID: from vae_mnist_topo.ipynb, improved in topo_losses3.ipynb

Note: the losses loss_topo0, 1, dsigma1, loss_persentropy1, return: (loss, got_loss) (1 if got the loss).

Build VAE:
"""

#from __future__ import print_function
import argparse
import torch
import torch.utils.data
from torch import nn, optim
from torch.nn import functional as F
from torchvision import datasets, transforms
from torchvision.utils import save_image
import torchvision.utils
import matplotlib.pyplot as plt

from torchvision.transforms.functional import to_pil_image

'''
parser = argparse.ArgumentParser(description='VAE MNIST Example')
parser.add_argument('--batch-size', type=int, default=128, metavar='N',
                    help='input batch size for training (default: 128)')
parser.add_argument('--epochs', type=int, default=10, metavar='N',
                    help='number of epochs to train (default: 10)')
parser.add_argument('--no-cuda', action='store_true', default=False,
                    help='disables CUDA training')
parser.add_argument('--no-mps', action='store_true', default=False,
                        help='disables macOS GPU training')
parser.add_argument('--seed', type=int, default=1, metavar='S',
                    help='random seed (default: 1)')
parser.add_argument('--log-interval', type=int, default=10, metavar='N',
                    help='how many batches to wait before logging training status')
'''
seed = 1
batch_size = 128
epochs = 30
log_interval = 50
torch.manual_seed(seed)
img_size = 28*28

device = "cuda" if torch.cuda.is_available() else "cpu"

'''
train_loader = torch.utils.data.DataLoader(
    datasets.MNIST('../data', train=True, download=True,
                   transform=transforms.ToTensor()),
    batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(
    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),
    batch_size=batch_size, shuffle=False)
'''

class VAE(nn.Module):
    def __init__(self, n_latent):
        super(VAE, self).__init__()
        '''
        self.fc11 = nn.Linear(img_size, 400)
        self.fc12 = nn.Linear(400, 200)
        self.fc21 = nn.Linear(200, n_latent)
        self.fc22 = nn.Linear(200, n_latent)
        self.fc3 = nn.Linear(n_latent, 200)
        self.fc41 = nn.Linear(200, 400)
        self.fc42 = nn.Linear(400, img_size)
        '''

        self.fc1 = nn.Linear(img_size, 400)
        self.fc21 = nn.Linear(400, n_latent)
        self.fc22 = nn.Linear(400, n_latent)
        self.fc3 = nn.Linear(n_latent, 400)
        self.fc4 = nn.Linear(400, img_size)

    # made for this structure of vae:
    def encode(self, x):
        h1 = F.relu(self.fc1(x))
        return self.fc21(h1), self.fc22(h1)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5*logvar)
        eps = torch.randn_like(std)
        return mu + eps*std

    def decode(self, z):
        h3 = F.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h3))

    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, img_size))
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

    '''
    def encode(self, x):
        h1 = F.relu(self.fc12(F.relu(self.fc11(x))))
        return self.fc21(h1), self.fc22(h1)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5*logvar)
        eps = torch.randn_like(std)
        return mu + eps*std #returns a single z

    def decode(self, z):
        h3 = F.relu(self.fc41(F.relu(self.fc3(z))))
        return torch.sigmoid(self.fc42(h3))

    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, img_size))
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar
    '''

'''
model = VAE()#.to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-3)
'''

"""Standard loss fctn and regularizers:"""

def loss_vae0(recon_x, x, mu, logvar):
    BCE = F.binary_cross_entropy(recon_x, x.view(-1, img_size), reduction='sum') #recon_x: fake batch of imgs, x: real batch of imgs

    # see Appendix B from VAE paper:
    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014
    # https://arxiv.org/abs/1312.6114
    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

    return BCE + KLD

fids = []

def loss_fctn1(recon_x, x, mu, logvar, dgm, dgm2): #bottleneck0
    w_vae = 1.
    w_topo0 = 10.
    ## normal loss:
    loss = w_vae * loss_vae0(recon_x, x, mu, logvar)
    ## compute topological loss:
    l_topo0, got_loss0 = d_bottleneck0(recon_x, dgm, dgm2)

    if got_loss0==1: loss += l_topo0 * w_topo0

    return loss

def loss_fctn2(recon_x, x, mu, logvar, dgm, dgm2): #bottleneck0+1
    w_vae = 1.
    w_topo0 = 0.05
    w_topo1 = 0.05
    ## normal loss:
    loss = w_vae * loss_vae0(recon_x, x, mu, logvar)
    ## compute topological loss:
    l_topo0, got_loss0 = d_bottleneck0(recon_x, dgm, dgm2) #loss of degree 0
    l_topo1, got_loss1 = d_bottleneck1(recon_x, dgm, dgm2) #loss of degree 1

    if got_loss0==1: loss += l_topo0 * w_topo0
    if got_loss1==1: loss += l_topo1 * w_topo1

    return loss

def loss_fctn3(recon_x, x, mu, logvar, dgm, dgm2): #pers entropy0+1. weights: 0.5, 0.5: does not learn. 0.3, 0.1: good
    w_vae = 1.
    w_topo0 = 0.2
    w_topo1 = 0.05
    ## normal loss:
    loss = w_vae * loss_vae0(recon_x, x, mu, logvar)
    ## compute topological loss:
    l_topo0 = loss_persentropy0(recon_x, dgm, dgm2) #loss of degree 0
    l_topo1, got_loss1 = loss_persentropy1(recon_x, dgm, dgm2) #loss of degree 1

    loss += l_topo0 * w_topo0
    if got_loss1==1: loss += l_topo1 * w_topo1

    return loss

def loss_fctn4(recon_x, x, mu, logvar, dgm, dgm2): #pers entropy0+dsigma1. 0.5, 0.5: does not learn. 0.1, 0.3: relatively good.
    w_vae = 1.
    w_topo0 = 0.1
    w_topo1 = 0.1
    ## normal loss:
    loss = w_vae * loss_vae0(recon_x, x, mu, logvar)
    ## compute topological loss:
    l_topo0 = loss_persentropy0(recon_x, dgm, dgm2) #loss of degree 0
    l_topo1, got_loss1 = dsigma1(recon_x, x.view(-1, img_size), dgm, dgm2) #loss of degree 1

    loss += l_topo0 * w_topo0
    if got_loss1==1: loss += l_topo1 * w_topo1

    return loss

def loss_fctn5(recon_x, x, mu, logvar, dgm, dgm2): # density0+dsigma1
    w_vae = 1.
    w_topo0 = 1.
    w_topo1 = 1.
    ## normal loss:
    loss = w_vae * loss_vae0(recon_x, x, mu, logvar)
    ## compute topological loss:
    l_topo0 = loss_density(recon_x, x.view(-1, img_size), dgm, dgm2, 0.2, 0.002, 2., 30, False) #loss of degree 0
    l_topo1, got_loss1 = dsigma1(recon_x, x.view(-1, img_size), dgm, dgm2) #loss of degree 1

    loss += l_topo0 * w_topo0
    if got_loss1==1: loss += l_topo1 * w_topo1

    return loss

def loss_fctn6(recon_x, x, mu, logvar, dgm, dgm2): # dsigma0+dsigma1
    w_vae = 1.
    w_topo0 = 0.1
    w_topo1 = 0.1
    ## normal loss:
    loss = w_vae * loss_vae0(recon_x, x, mu, logvar)
    ## compute topological loss:
    l_topo0 = dsigma0(recon_x, x.view(-1, img_size), dgm, dgm2) #loss of degree 0
    l_topo1, got_loss1 = dsigma1(recon_x, x.view(-1, img_size), dgm, dgm2) #loss of degree 1

    loss += l_topo0 * w_topo0
    if got_loss1==1: loss += l_topo1 * w_topo1

    return loss

"""Download MNIST dataset:"""

from torchvision.datasets import MNIST

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# Download the MNIST dataset without splitting into training and test sets
mnist_dataset = MNIST(root='./data', train=True, transform=transform, download=True)

print(mnist_dataset)

"""Train the 7 models together to compare the models:"""

def plot_batch(imgs):
  imgs2 = imgs.reshape(-1, 1, 28, 28)
  grid_img = torchvision.utils.make_grid(imgs2[:32], nrow=8, normalize=True)  # Create a grid of images
  # Convert the grid tensor to numpy array and transpose the dimensions
  grid_img = grid_img.cpu().numpy().transpose((1, 2, 0))
  # Display the grid of images
  plt.figure(figsize=(10, 10))
  plt.imshow(grid_img)
  plt.axis('off')
  plt.show()

batch_size = 128 # 128
epochs = 14
n_showplots = 50

#batchsize 128: 3min for 10 iterations -> 140min for 1 epoch. batchsize 64: 58s for 10 iterations -> 94min for 1 epoch. with dgms of batches64 precomputed: 51s for 10 iterations

train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)

def plotdgm(dgm):
  dgm_gtda = _postprocess_diagrams([dgm["dgms"]], "ripser", (0,1), np.inf, True)[0]
  fig = go.Figure(plot_diagram(dgm_gtda, homology_dimensions=(0,1)))
  fig.show()

n_batches = 0
dgms_batches = []

for batch_idx, (data, _) in enumerate(train_loader):
  data = data.view(data.size(0), -1)
  points_np = data.view(-1, img_size).numpy()
  # if batch_idx==200: plot_batch(data)
  dgm2 = ripser_parallel(points_np, maxdim=1, return_generators=True)
  dgms_batches.append(dgm2)
  #plotdgm(dgm2)
  n_batches += 1

print(n_batches)

#create the 6 models (model0 is normal VAE, models1-6 have topo losses):
epochs = 30
n_latent = 10
model0 = VAE(n_latent)
model1 = VAE(n_latent)
model2 = VAE(n_latent)
model3 = VAE(n_latent)
model4 = VAE(n_latent)
model5 = VAE(n_latent)
model6 = VAE(n_latent)
optimizer0 = optim.Adam(model0.parameters(), lr=1e-3)
optimizer1 = optim.Adam(model1.parameters(), lr=1e-3)
optimizer2 = optim.Adam(model2.parameters(), lr=1e-3)
optimizer3 = optim.Adam(model3.parameters(), lr=1e-3)
optimizer4 = optim.Adam(model4.parameters(), lr=1e-3)
optimizer5 = optim.Adam(model5.parameters(), lr=1e-3)
optimizer6 = optim.Adam(model6.parameters(), lr=1e-3)
model0.train()
model1.train()
model2.train()
model3.train()
model4.train()
model5.train()
model6.train()

for epoch in range(epochs):
    for batch_idx, (data, _) in enumerate(train_loader):
        if batch_idx==1: print("done")
        if batch_idx==10: print("done2")
        data = data.view(data.size(0), -1)
        if batch_idx % n_showplots != 0 or batch_idx==0:
          #get dgm2:
          '''
          points_np = data.view(-1, img_size).numpy()
          dgm2 = ripser_parallel(points_np, maxdim=1, return_generators=True)
          '''
          dgm2 = dgms_batches[batch_idx]

          #update the 4 models:
          optimizer0.zero_grad()
          optimizer1.zero_grad()
          optimizer2.zero_grad()
          optimizer3.zero_grad()
          optimizer4.zero_grad()
          optimizer5.zero_grad()
          optimizer6.zero_grad()

          recon_batch, mean, log_var = model0(data)
          loss = loss_vae0(recon_batch, data, mean, log_var)
          loss.backward()
          optimizer0.step()

          recon_batch, mean, log_var = model1(data)
          dgm = get_dgm(recon_batch, 0)
          loss = loss_fctn1(recon_batch, data, mean, log_var, dgm, dgm2)
          loss.backward()
          optimizer1.step()

          recon_batch, mean, log_var = model2(data)
          dgm = get_dgm(recon_batch, 1)
          loss = loss_fctn2(recon_batch, data, mean, log_var, dgm, dgm2)
          loss.backward()
          optimizer2.step()

          recon_batch, mean, log_var = model3(data)
          dgm = get_dgm(recon_batch, 1)
          loss = loss_fctn3(recon_batch, data, mean, log_var, dgm, dgm2)
          loss.backward()
          optimizer3.step()

          recon_batch, mean, log_var = model4(data)
          dgm = get_dgm(recon_batch, 1)
          loss = loss_fctn4(recon_batch, data, mean, log_var, dgm, dgm2)
          loss.backward()
          optimizer4.step()

          recon_batch, mean, log_var = model5(data)
          dgm = get_dgm(recon_batch, 1)
          loss = loss_fctn5(recon_batch, data, mean, log_var, dgm, dgm2)
          loss.backward()
          optimizer5.step()

          recon_batch, mean, log_var = model6(data)
          dgm = get_dgm(recon_batch, 1)
          loss = loss_fctn6(recon_batch, data, mean, log_var, dgm, dgm2)
          loss.backward()
          optimizer6.step()

          if batch_idx % n_showplots == 1 and batch_idx > 100:
            print(f"Epoch {epoch+1}/{epochs} - Batch {batch_idx}/{len(train_loader)}")
            print("Input: real data (trained on)")
            with torch.no_grad():
                print("Real batch:")
                plot_batch(data)

                print("Model 0:")
                recon_batch, _, _ = model0(data)
                plot_batch(recon_batch)

                print("Model 1:")
                recon_batch, _, _ = model1(data)
                plot_batch(recon_batch)

                print("Model 2:")
                recon_batch, _, _ = model2(data)
                plot_batch(recon_batch)

                print("Model 3:")
                recon_batch, _, _ = model3(data)
                plot_batch(recon_batch)

                print("Model 4:")
                recon_batch, _, _ = model4(data)
                plot_batch(recon_batch)

                print("Model 5:")
                recon_batch, _, _ = model5(data)
                plot_batch(recon_batch)

                print("Model 6:")
                recon_batch, _, _ = model6(data)
                plot_batch(recon_batch)

        else: #ie batch_idx % n_showplots == 0 and >0:
            print(f"Epoch {epoch+1}/{epochs} - Batch {batch_idx}/{len(train_loader)}")
            print("Input: new data (not trained on) and input random latent vectors:")

            with torch.no_grad():
                print("Real batch:")
                plot_batch(data)

                sampled_latent = torch.randn(32, n_latent)
                #show imgs generated by the 7 models:

                print("Model 0:")
                recon_batch, _, _ = model0(data)
                plot_batch(recon_batch)
                generated_samples = model0.decode(sampled_latent)
                plot_batch(generated_samples)

                print("Model 1:")
                recon_batch, _, _ = model1(data)
                plot_batch(recon_batch)
                generated_samples = model1.decode(sampled_latent)
                plot_batch(generated_samples)
                '''
                generated_samples = model1.decode(sampled_latent).reshape(-1, 1, 28, 28)#.detach()#.numpy()
                grid_img = torchvision.utils.make_grid(generated_samples[:32], nrow=8, normalize=True)  # Create a grid of images
                # Convert the grid tensor to numpy array and transpose the dimensions
                grid_img = grid_img.cpu().numpy().transpose((1, 2, 0))
                # Display the grid of images
                plt.figure(figsize=(10, 10))
                plt.imshow(grid_img)
                plt.axis('off')
                plt.show()
                '''

                print("Model 2:")
                recon_batch, _, _ = model2(data)
                plot_batch(recon_batch)
                generated_samples = model2.decode(sampled_latent)
                plot_batch(generated_samples)

                print("Model 3:")
                recon_batch, _, _ = model3(data)
                plot_batch(recon_batch)
                generated_samples = model3.decode(sampled_latent)
                plot_batch(generated_samples)

                print("Model 4:")
                recon_batch, _, _ = model4(data)
                plot_batch(recon_batch)
                generated_samples = model4.decode(sampled_latent)
                plot_batch(generated_samples)

                print("Model 5:")
                recon_batch, _, _ = model5(data)
                plot_batch(recon_batch)
                generated_samples = model5.decode(sampled_latent)
                plot_batch(generated_samples)

                print("Model 6:")
                recon_batch, _, _ = model6(data)
                plot_batch(recon_batch)
                generated_samples = model6.decode(sampled_latent)
                plot_batch(generated_samples)

"""Train only the normal VAE (no topo losses):"""

#model0 is normal VAE, models1-6 have topo losses:
n_latent = 10
model0 = VAE(n_latent)
optimizer0 = optim.Adam(model0.parameters(), lr=1e-3)
model0.train()
n_showplots = 200

for epoch in range(epochs):
    for batch_idx, (data, _) in enumerate(train_loader):
        if batch_idx==1: print("done")
        if batch_idx==10: print("done2")
        data = data.view(data.size(0), -1)
        if batch_idx % n_showplots != 0 or batch_idx==0:
          optimizer0.zero_grad()

          recon_batch, mean, log_var = model0(data)
          loss = loss_vae0(recon_batch, data, mean, log_var)
          loss.backward()
          optimizer0.step()

          if batch_idx % n_showplots == 1 and batch_idx > 100:
            print(f"Epoch {epoch+1}/{epochs} - Batch {batch_idx}/{len(train_loader)}")
            print("Input: real data (trained on)")
            with torch.no_grad():
                print("Real batch:")
                plot_batch(data)

                print("Model 0:")
                recon_batch, _, _ = model0(data)
                plot_batch(recon_batch)

        else: #ie batch_idx % n_showplots == 0 and >0:
            print(f"Epoch {epoch+1}/{epochs} - Batch {batch_idx}/{len(train_loader)}")
            print("Input: new data (not trained on) and input random latent vectors:")

            with torch.no_grad():
                print("Real batch:")
                plot_batch(data)

                sampled_latent = torch.randn(32, n_latent)
                #show imgs generated by the 7 models:

                print("Model 0:")
                recon_batch, _, _ = model0(data)
                plot_batch(recon_batch)
                generated_samples = model0.decode(sampled_latent)
                plot_batch(generated_samples)

