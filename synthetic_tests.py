# -*- coding: utf-8 -*-
"""synthetic_tests.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gxlWykplx8qqInXbS17W8M96p8eaiaTM
"""

# ripser_parallel: https://giotto-ai.github.io/giotto-ph/build/html/modules/ripser_parallel.html
# bottleneck dist: https://persim.scikit-tda.org/en/latest/notebooks/distances.html

!pip3 install giotto-ph
import numpy as np
from gph import ripser_parallel

!pip install ipython

from IPython.display import Image  # to display images
import sys
!{sys.executable} -m pip install giotto-tda

from gph import ripser_parallel

# Import utils
import numpy as np
from gtda.homology._utils import _postprocess_diagrams

# To generate dataset
from sklearn import datasets

# Plotting
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from plotly import graph_objects as go
from gtda.plotting import plot_diagram, plot_point_cloud

!pip3 install Cython ripser tadasets
!pip3 install persim #provides matching
#used this: https://persim.scikit-tda.org/en/latest/notebooks/distances.html
import persim
import tadasets
import ripser
#other packages:
import math
import scipy
import torch
import random

#euclidean dist for torch tensors:
def dist(point1, point2):
    return torch.sqrt(torch.sum((point2 - point1)**2))

#euclidean dist for numpy points:
def dist_np(point1, point2):
    return np.sqrt(np.sum((point2 - point1)**2))

#supremum dist for torch tensors:
def dist_sup_tc(b1, d1, b2, d2):
    # Calculate the sup norm between points (b1, d1) and (b2, d2)
    return torch.max(torch.abs(b1 - b2), torch.abs(d1 - d2))

#auxiliary loss when d(D,D0) (in deg0) only depends on D0 (so gradients are 0):
def push0(point_cloud):
    with torch.no_grad():
        ##convert points for computing PD:
        points_np = point_cloud.numpy()
        # get PD with generators:
        dgm = ripser_parallel(points_np, maxdim=0, return_generators=True)

    p1, p2 = point_cloud[dgm['gens'][0][0][1]], point_cloud[dgm['gens'][0][0][2]]
    loss = -torch.abs(dist(p1, p2))/2.
    for i in range(1, len(dgm['gens'][0])):
      p1, p2 = point_cloud[dgm['gens'][0][i][1]], point_cloud[dgm['gens'][0][i][2]] #pt (0,d) with d=dist(p1,p2) (euclidean dist)
      loss += -torch.abs(dist(p1, p2))/2. #dist to diagonal of (0,d) is d/2

    return loss

#auxiliary loss when d(D,D0) (in deg1) only depends on D0 (so gradients are 0):
def push1(point_cloud, furthest_p2):
    with torch.no_grad():
        ##convert points for computing PD:
        points_np = point_cloud.numpy()
        # get PD with generators:
        dgm = ripser_parallel(points_np, maxdim=1, return_generators=True)

        distances_to_furthest_p2 = [dist_np(furthest_p2, dgm['dgms'][1][i]) for i in range(len(dgm['dgms'][1]))]
        fp = np.argmin(distances_to_furthest_p2) #fp: index of closest off-diag point to the point of PD2 that gives the PD

    p1, p2, p3, p4 = point_cloud[dgm['gens'][1][0][fp][0]], point_cloud[dgm['gens'][1][0][fp][1]], point_cloud[dgm['gens'][1][0][fp][2]], point_cloud[dgm['gens'][1][0][fp][3]]

    ##the closes pt to furthest_pt is (b,d) with b=dist(p1,p2) and d=dist(p3,p4) -> minimize dist((b,d), furthest_p2)
    furthest_p2 = torch.tensor(furthest_p2)
    loss = torch.sqrt((dist(p1, p2) - furthest_p2[0])**2 + (dist(p3, p4) - furthest_p2[1])**2)

    return loss

def d_bottleneck0(point_cloud, dgm, dgm2): # got_loss=1 if got loss, =0 if loss does not depend on dgm
    got_loss = 1
    with torch.no_grad():
        distance_bottleneck, matching = persim.bottleneck(dgm['dgms'][0][:-1], dgm2['dgms'][0][:-1], matching=True)
        #find the pair that gives the max distance:
        index = np.argmax(matching[:, 2])
        i, j = int(matching[index][0]), int(matching[index][1]) #i, j: the i-th and j-th point of the dgm1, dgm2 respectively, that give the bottleneck dist.
        # (if the largest dist is point<->diagonal: i or j is -1)
        #i is the i-th pt in dgm and j is the j-th pt in dgm2 which give the bottleneck dist (i.e. it is the largest dim)
        #for the loss, need to know what is the point i (learnable), i=(distmatrix[xi,yi],distmatrix[ai,bi]) in the distance matrix for some 4 indices
        #but gen[0]
        # i is the index of a point of the PD. but (gens[i][1], gens[i][2]) is the pair of vertices of the point cloud that correspond to the point i=(0,d), with d=dist(gens[i][1]-gens[i][2])
        #get the 2 points that give the distance of the i-th pt in dgm in the 1st diagram and compute the loss:
    if i>=0:
      point1_dgm1 = point_cloud[dgm['gens'][0][i][1]]
      point2_dgm1 = point_cloud[dgm['gens'][0][i][2]]

    if i>=0 and j>=0:
      loss = torch.abs(dist(point1_dgm1, point2_dgm1) - dgm2['dgms'][0][j][1])
    else:
      if i==-1: #so the j-th point from dgm2 is matched to the diagonal -> backprop through loss would give 0 -> goal: make points further from diag
        #new_bdist = torch.abs(dist(point1_dgm2, point2_dgm2) - 0.)/2
        loss = 0
        got_loss = 0
      else: #then  j==-1, so the i-th point from dgm1 is matched to the diagonal
        loss = dist(point1_dgm1, point2_dgm1)/2.

    return loss, got_loss

def d_bottleneck1(point_cloud, dgm, dgm2): # got_loss=1 if got loss, =0 if loss does not depend on dgm
    got_loss = 1
    if len(dgm['dgms'][1])==0: return 0, 0
    if len(dgm2['dgms'][1])==0: dgm2['dgms'][1] = [[0.,0.]] #print("error2")
    with torch.no_grad():
        distance_bottleneck, matching = persim.bottleneck(dgm['dgms'][1], dgm2['dgms'][1], matching=True)
        #find the pair that gives the max distance:
        index = np.argmax(matching[:, 2])
        i, j = int(matching[index][0]), int(matching[index][1])
        #i is the i-th pt in dgm and j is the j-th pt in dgm2 which give the bottleneck dist (i.e. it is the largest dim)
        #for the loss, need to know what is the point i (learnable), i=(distmatrix[xi,yi],distmatrix[ai,bi]) in the distance matrix for some 4 indices
        # i is the index of a point of the PD. but (gens[i][1], gens[i][2]) is the pair of vertices of the point cloud that correspond to the point i=(0,d), with d=dist(gens[i][1]-gens[i][2])

    #get the 2 points that give the distance of the i-th pt in dgm in the 1st diagram:
    #if i>0, then the pt of dgm1 is off-diag:
    if i>=0:
      point0_dgm1 = point_cloud[dgm['gens'][1][0][i][0]]
      point1_dgm1 = point_cloud[dgm['gens'][1][0][i][1]]
      point2_dgm1 = point_cloud[dgm['gens'][1][0][i][2]]
      point3_dgm1 = point_cloud[dgm['gens'][1][0][i][3]]
      birth_dgm1 = dist(point0_dgm1, point1_dgm1)
      death_dgm1 = dist(point2_dgm1, point3_dgm1)
    #get the 2 points that give the distance of the j-th pt in dgm in the 2nd diagram:
    if j>=0:
      birth_dgm2 = dgm2['dgms'][1][j][0]
      death_dgm2 = dgm2['dgms'][1][j][1]

    if i>=0 and j>=0:
      loss = dist_sup_tc(birth_dgm1, death_dgm1, birth_dgm2, death_dgm2)
    else:
      if i==-1: #so the j-th point from dgm2 is matched to the diagonal
        loss = 0
        got_loss = 0
      else: #then j==-1, so the i-th point from dgm1 is matched to the diagonal
        loss = (death_dgm1 - birth_dgm1)/2.

    return loss, got_loss

def dist_2(a, b, c, d):
    return (a - c)**2 + (b - d)**2

#return Reininghaus kernel ksigma: (could make it slightly faster with different functions for each dgm (dgm2 does not need backpropagation)), but let it same for all dgms
def ksigma0(point_cloud, point_cloud2, dgm, dgm2): #maxdim of both dgms: 0
    sigma = 0.05
    ksigma = 0
    ## use formula for k_sigma from paper (https://arxiv.org/pdf/1412.6821.pdf):
    for i in range(len(dgm['gens'][0])):
        # pt in dgm: (0,d), d=dist(p1,p2)
        p1, p2 = point_cloud[dgm['gens'][0][i][1]], point_cloud[dgm['gens'][0][i][2]]
        d1 = dist(p1, p2)
        for j in range(len(dgm2['gens'][0])):
           #pt in dgm2: (0,d), d=dist(q1,q2)
           q1, q2 = point_cloud2[dgm2['gens'][0][j][1]], point_cloud2[dgm2['gens'][0][j][2]]
           d2 = dist(q1, q2)
           ksigma += torch.exp(-dist_2(0, d1, 0, d2)/(8*sigma)) - torch.exp(-dist_2(0, d1, d2, 0)/(8*sigma))

    ksigma *= 1/(8*3.141592*sigma)
    return ksigma

#return pseudo-distance that comes from ksigma and squared, dsigma**2:
def dsigma0(point_cloud, point_cloud2, dgm, dgm2):
    k11 = ksigma0(point_cloud, point_cloud, dgm, dgm)
    #k22 = ksigma(point_cloud2, point_cloud2)
    k12 = ksigma0(point_cloud, point_cloud2, dgm, dgm2)
    #return k11 + k22 - 2*k12
    return k11 - 2*k12 #no need of k22 since no backpropagation through it (fixed point cloud)

#the only diff from ksigma (deg0) is how to take the pt of dgms (b,d) wrt the pts of the point clouds, for the backpropagation
def ksigma1(point_cloud, point_cloud2, dgm, dgm2): #maxdim of both dgms: 1
    sigma = 0.05
    if len(dgm['dgms'][1])==0 or len(dgm['dgms'][1])==0: return 0, 0

    ksigma1 = 0
    ## use formula for k_sigma from paper (https://arxiv.org/pdf/1412.6821.pdf):
    for i in range(len(dgm['gens'][1])):
        # pt in dgm: (b1,d1), with b1, d1 = dist(p2, p1), dist(dist(p3, p4)
        p1, p2, p3, p4 = point_cloud[dgm['gens'][1][0][i][0]], point_cloud[dgm['gens'][1][0][i][1]], point_cloud[dgm['gens'][1][0][i][2]], point_cloud[dgm['gens'][1][0][i][3]]
        b1 = dist(p1,p2)
        d1 = dist(p3,p4)

        for j in range(len(dgm2['gens'][1])):
          #pt in dgm2: (b2,d2)
          q1, q2, q3, q4 = point_cloud2[dgm2['gens'][1][0][j][0]], point_cloud2[dgm2['gens'][1][0][j][1]], point_cloud2[dgm2['gens'][1][0][j][2]], point_cloud2[dgm2['gens'][1][0][j][3]]
          b2 = dist(q1,q2)
          d2 = dist(q3,q4)

          ksigma1 += torch.exp(-dist_2(b1, d1, b2, d2)/(8*sigma)) - torch.exp(-dist_2(b1, d1, d2, b2)/(8*sigma))

    ksigma1 *= 1/(8*3.141592*sigma)
    return ksigma1, 1

def dsigma1(point_cloud, point_cloud2, dgm, dgm2):
    k12, gotloss = ksigma1(point_cloud, point_cloud2, dgm, dgm2)
    if gotloss == 0: return 0, 0
    # if both dgm and dgm2 have at least 1 point:
    k11, gotloss = ksigma1(point_cloud, point_cloud, dgm, dgm)
    return k11 + (-2.) * k12, 1

def density(point_cloud, dgm, sigma, scale, x):
  tot = 0
  density_x = 0 #density at x
  for i in range(len(dgm['dgms'][0])-1):
    p1, p2 = point_cloud[dgm['gens'][0][i][1]], point_cloud[dgm['gens'][0][i][2]] #pt (0,d) with d=dist(p1,p2) (euclidean dist)
    d = dist(p1, p2) #pt of pt cloud is (0,d)
    density_x += d**4 * torch.exp(-((d-x)/sigma)**2)

  return density_x * scale

def loss_density(point_cloud, point_cloud2, dgm, dgm2, sigma, scale, maxrange, npoints, plot): #dgm of deg0
  xs = np.linspace(0., maxrange, npoints)
  loss = 0
  ## compute difference between both functions in 100 pts (those given by xs)
  dens=[]
  dens0=[]
  for x in xs:
    dx = density(point_cloud, dgm, sigma, scale, x)
    d0x = density(point_cloud2, dgm2, sigma, scale, x)
    loss += (dx - d0x)**2
    if plot:
      with torch.no_grad():
        dens.append(dx.detach().numpy())
        dens0.append(d0x.detach().numpy())

  if plot: #plot both density functions (blue: the one of dgm being learned)
    plt.plot(xs, dens, color='blue')
    plt.plot(xs, dens0, color='red')
    plt.show()

  return loss

## use: sigma=0.2, scale=0.002, maxrange=35., npoints=100

def loss_persentropy0(point_cloud, dgm, dgm2, delta): #dgm of deg0. only looks at points with pers=|d|>delta (for avoiding large gradients) (in both dgms)
  L = 0
  for i in range(len(dgm['dgms'][0])-1):
    with torch.no_grad(): pers1 = dist(point_cloud[dgm['gens'][0][i][1]], point_cloud[dgm['gens'][0][i][2]])
    if pers1>delta: L += dist(point_cloud[dgm['gens'][0][i][1]], point_cloud[dgm['gens'][0][i][2]])

  with torch.no_grad():
    if L==0: return 0, 0

  pers = 0
  for i in range(len(dgm['dgms'][0])-1):
    p1, p2 = point_cloud[dgm['gens'][0][i][1]], point_cloud[dgm['gens'][0][i][2]] #pt (0,d) with d=dist(p1,p2) (euclidean dist)
    with torch.no_grad(): d1 = dist(p1, p2)
    if d1>delta: pers += dist(p1, p2) * torch.log(dist(p1, p2)/L) #pt of pt cloud is (0,dist(p1, p2))

  ##get pers entropy of dgm2:
  L2 = 0
  pers2 = 0
  for i in range(len(dgm2['dgms'][0])-1):
    if dgm2['dgms'][0][i][1] > delta: L2 += dgm2['dgms'][0][i][1]
  if L2==0: return (pers/L)**2, 1
  for i in range(len(dgm2['dgms'][0])-1):
    if dgm2['dgms'][0][i][1] > delta: pers2 += dgm2['dgms'][0][i][1] * math.log(dgm2['dgms'][0][i][1] / L2)

  return (pers/L - pers2/L2)**2, 1

def loss_persentropy1(point_cloud, dgm, dgm2, delta): #dgm of deg1. returns loss, got_loss (0 if did not get it). only looks at points with pers=|d-b|>delta (in both dgms) (for avoiding large gradients)
  if len(dgm['dgms'][1])==0: return 0, 0 #no loss if dgm has no off-diag points

  #entropy of dgm:
  L = 0
  for i in range(len(dgm['dgms'][1])):
        # pt in dgm: (b1,d1), with b1, d1 = dist(p2, p1), dist(dist(p3, p4)
        p1, p2, p3, p4 = point_cloud[dgm['gens'][1][0][i][0]], point_cloud[dgm['gens'][1][0][i][1]], point_cloud[dgm['gens'][1][0][i][2]], point_cloud[dgm['gens'][1][0][i][3]]
        b1 = dist(p1,p2)
        d1 = dist(p3,p4)
        with torch.no_grad(): pers1 = d1-b1
        if pers1 > delta: L += d1 - b1

  with torch.no_grad():
    if L==0: return 0, 0

  pers = 0
  for i in range(len(dgm['gens'][1])):
        # pt in dgm: (b1,d1), with b1, d1 = dist(p2, p1), dist(dist(p3, p4)
        p1, p2, p3, p4 = point_cloud[dgm['gens'][1][0][i][0]], point_cloud[dgm['gens'][1][0][i][1]], point_cloud[dgm['gens'][1][0][i][2]], point_cloud[dgm['gens'][1][0][i][3]]
        b1 = dist(p1,p2)
        d1 = dist(p3,p4)
        with torch.no_grad(): pers1 = d1-b1
        if pers1 > delta: pers += (d1-b1) * torch.log((d1-b1)/L)

  if len(dgm2['dgms'][1])==0: return (pers/L)**2, 1 # the pers of dgm2 is taken as 0

  #entropy of dgm2:
  L2 = 0
  for i in range(len(dgm2['dgms'][1])):
        if dgm2['dgms'][1][i][1] - dgm2['dgms'][1][i][0]>delta: L2 += dgm2['dgms'][1][i][1] - dgm2['dgms'][1][i][0]

  if L2==0: return (pers/L)**2, 1 # the pers of dgm2 is taken as 0
  pers2 = 0
  for i in range(len(dgm2['dgms'][1])):
        lifespan = dgm2['dgms'][1][i][1] - dgm2['dgms'][1][i][0]
        if lifespan>delta: pers2 += torch.tensor(lifespan) * torch.log(torch.tensor(lifespan/L2))

  return (pers/L - pers2/L2)**2, 1

!pip install pillow

import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
from IPython.display import Image as IPImage

def plot_pc_gif(point_cloud):
    fig = plt.figure(figsize=(6, 6))
    plt.scatter(point_cloud[:, 0], point_cloud[:, 1], s=10, c='b')
    #plt.xlabel('X')
    plt.xlim(-10, 20)  # Adjust the limits as per your point cloud data
    plt.ylim(-5, 30)  # Adjust the limits as per your point cloud data
    plt.close(fig)
    return fig

def generate_gif(point_clouds):
    # Create a list of figures for each point cloud
    figures = [plot_pc_gif(point_cloud) for point_cloud in point_clouds]

    # Save each figure as an image and store them in a list
    images = []
    for idx, fig in enumerate(figures):
        fig.savefig(f'point_cloud_{idx}.png', dpi=80)
        images.append(Image.open(f'point_cloud_{idx}.png'))

    # Save the images as a GIF
    images[0].save('point_clouds_evolution.gif', save_all=True, append_images=images[1:], duration=50, loop=0) # 70 for test3

    # Display the GIF
    IPImage('point_clouds_evolution.gif')

"""Total topological losses:"""

def get_dgm(point_cloud, deg):
  # Compute the persistence diagram without backprop
  with torch.no_grad():
        ##convert points for computing PD:
        points_np = point_cloud.numpy()
        # get PD with generators:
        dgm = ripser_parallel(points_np, maxdim=deg, return_generators=True)
  return dgm

def loss_topo0(point_cloud):
  dgm = get_dgm(point_cloud, 1)

  l_topo0, got_loss0 = d_bottleneck0(point_cloud, dgm, dgm2)
  l_topo1, got_loss1 = d_bottleneck1(point_cloud, dgm, dgm2)
  if got_loss0==1 or got_loss1==1: return l_topo0 + l_topo1, (l_topo0 + l_topo1).item()
  return push0(point_cloud), -1

def loss_topo1(point_cloud):
  dgm = get_dgm(point_cloud, 1)

  l_topo0, got_loss0 = loss_persentropy0(point_cloud, dgm, dgm2, 0.1)
  l_topo1, got_loss1 = loss_persentropy1(point_cloud, dgm, dgm2, 0.1)
  if got_loss0==1 or got_loss1==1: return l_topo0 + l_topo1, (l_topo0 + l_topo1).item()
  return push0(point_cloud), (l_topo0 + l_topo1).item()

"""Test 1: The learnable point cloud begins with 5 clusters, and the reference point cloud has 3 clusters."""

"""First, generate a snythetic ground truth point cloud with dgm:"""
##create point_cloud2 and dgm2:
point_cloud2 = np.array([[5.,5.], [10., 10.], [20.0, 6.0]])
# Plot the point cloud
fig = go.Figure(plot_point_cloud(point_cloud2))
fig.show()
dgm2 = ripser_parallel(point_cloud2, maxdim=1, return_generators=True)
dgm_gtda = _postprocess_diagrams([dgm2["dgms"]], "ripser", (0, 1), np.inf, True)[0]
fig = go.Figure(plot_diagram(dgm_gtda, homology_dimensions=(0, 1)))
fig.show()
point_cloud2 = torch.tensor(point_cloud2, dtype=torch.float32)

number_of_iterations = 15000
point_clouds = []
losses = []

## added for creating manually the initial point cloud:
number_of_points = 64
newdata = np.zeros((number_of_points,2))
r1 = 0.5
for i in range(10):
  newdata[i][0] = random.uniform(-r1, r1)
  newdata[i][1] = random.uniform(-r1, r1)

for i in range(10):
  newdata[i+10][0] = random.uniform(-r1, r1)+10.
  newdata[i+10][1] = random.uniform(-r1, r1)
for i in range(10):
  newdata[i+20][0] = random.uniform(-r1, r1)
  newdata[i+20][1] = random.uniform(-r1, r1)+20
for i in range(10):
  newdata[i+30][0] = random.uniform(-r1, r1)+30
  newdata[i+30][1] = random.uniform(-r1, r1)+30
for i in range(24):
  newdata[i+40][0] = random.uniform(-r1, r1)+10
  newdata[i+40][1] = random.uniform(-r1, r1)-25

point_cloud = torch.tensor(newdata, dtype=torch.float32, requires_grad=False)

# plot the PD initial:
dgm = ripser_parallel(newdata, maxdim=1, return_generators=True)
dgm_gtda = _postprocess_diagrams([dgm["dgms"]], "ripser", (0, 1), np.inf, True)[0]
fig = go.Figure(plot_diagram(dgm_gtda, homology_dimensions=(0,1)))
fig.show()

point_cloud.requires_grad = True
point_clouds.append(np.copy(point_cloud.detach().numpy()))
optimizer = torch.optim.Adam([point_cloud], lr=0.01)

#plot initial point cloud:
fig = go.Figure(plot_point_cloud(point_clouds[-1]))
fig.show()

for i in range(number_of_iterations):

    loss, bdist = loss_topo0(point_cloud)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    if i % 50 == 0 or i==number_of_iterations - 1: point_clouds.append(np.copy(point_cloud.detach().numpy()))

    if i % 1000 == 0 or i == number_of_iterations-1:
        print(f"Iteration {i + 1}/{number_of_iterations}, Loss: {loss.item()}")
        fig = go.Figure(plot_point_cloud(point_clouds[-1]))
        fig.show()

        # get PD of latest point cloud and plot it:
        dgm = ripser_parallel(point_clouds[-1], maxdim=1, return_generators=True)
        dgm_gtda = _postprocess_diagrams([dgm["dgms"]], "ripser", (0, 1), np.inf, True)[0]
        fig = go.Figure(plot_diagram(dgm_gtda, homology_dimensions=(0,1)))
        fig.show()
    if i % 5 == 0 or i==number_of_iterations - 1:
      with torch.no_grad(): losses.append(bdist)

xs = []
for i in range(len(losses)): xs.append(i*5)
plt.plot(xs, losses)
plt.xlabel("Iteration")
plt.ylabel("$L_1$")
plt.show()

generate_gif(point_clouds)

"""Test 2: The learnable point cloud begins with 2 clusters, and the reference point cloud has 4 clusters."""

"""First, generate a snythetic ground truth point cloud with dgm:"""

##create point_cloud2 and dgm2:
point_cloud2 = np.zeros((128,2))
r1 = 0.3
for i in range(30):
  point_cloud2[i][0] = random.uniform(-r1, r1)
  point_cloud2[i][1] = random.uniform(-r1, r1)
for i in range(30, 50):
  point_cloud2[i][0] = random.uniform(-r1, r1)+10.
  point_cloud2[i][1] = random.uniform(-r1, r1)
for i in range(50,80):
  point_cloud2[i][0] = random.uniform(-r1, r1)-5.
  point_cloud2[i][1] = random.uniform(-r1, r1)+4.
for i in range(80,128):
  point_cloud2[i][0] = random.uniform(-r1, r1)+8.
  point_cloud2[i][1] = random.uniform(-r1, r1)+13.

# Plot the point cloud
fig = go.Figure(plot_point_cloud(point_cloud2))
fig.show()

dgm2 = ripser_parallel(point_cloud2, maxdim=1, return_generators=True)
dgm_gtda = _postprocess_diagrams([dgm2["dgms"]], "ripser", (0,1 ), np.inf, True)[0]
fig = go.Figure(plot_diagram(dgm_gtda, homology_dimensions=(0,1)))
fig.show()

number_of_iterations = 2500
point_cloud2 = torch.tensor(point_cloud2, dtype=torch.float32)

point_clouds = []
losses2 = []

## added for creating manually the initial point cloud:
number_of_points = 64
newdata = np.zeros((number_of_points,2))
r1 = 0.4
for i in range(30):
  newdata[i][0] = random.uniform(-r1, r1)
  newdata[i][1] = random.uniform(-r1, r1)
for i in range(34):
  newdata[i+10][0] = random.uniform(-r1, r1)+10.
  newdata[i+10][1] = random.uniform(-r1, r1)+5.

point_cloud = torch.tensor(newdata, dtype=torch.float32, requires_grad=False)

# plot the PD initial:
dgm = ripser_parallel(newdata, maxdim=1, return_generators=True)
dgm_gtda = _postprocess_diagrams([dgm["dgms"]], "ripser", (0,1), np.inf, True)[0]
fig = go.Figure(plot_diagram(dgm_gtda, homology_dimensions=(0,1)))
fig.show()

point_cloud.requires_grad = True
point_clouds.append(np.copy(point_cloud.detach().numpy()))
optimizer = torch.optim.Adam([point_cloud], lr=0.05)

#plot initial point cloud:
fig = go.Figure(plot_point_cloud(point_clouds[-1]))
fig.show()

for i in range(number_of_iterations):
    loss, bdist = loss_topo0(point_cloud)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    if i % 25 == 0 or i==number_of_iterations - 1:  point_clouds.append(np.copy(point_cloud.detach().numpy()))

    if i == 0 or i == number_of_iterations-1:
        print(f"Iteration {i + 1}/{number_of_iterations}, Loss: {loss.item()}")
        fig = go.Figure(plot_point_cloud(point_clouds[-1]))
        fig.show()

        # get PD of latest point cloud and plot it:
        dgm = ripser_parallel(point_clouds[-1], maxdim=1, return_generators=True)
        dgm_gtda = _postprocess_diagrams([dgm["dgms"]], "ripser", (0, 1), np.inf, True)[0]
        fig = go.Figure(plot_diagram(dgm_gtda, homology_dimensions=(0,1)))
        fig.show()
    if i % 5 == 0 or i==number_of_iterations - 1:
      with torch.no_grad(): losses2.append(bdist)

xs = []
for i in range(len(losses2)): xs.append(i*5)
plt.scatter(xs, losses2, s=8)
plt.xlabel("Iteration")
plt.xlim([0,2500])
plt.ylim([0,5])
plt.ylabel("$L_1$")
plt.show()

generate_gif(point_clouds)

"""Test 3: The learnable point cloud begins as two lines, and the reference point cloud is a circle."""

## create ground truth dgm2:
# Plot the point cloud
point_cloud2 = tadasets.dsphere(d=1, n=100, noise=0.0)
point_cloud2 *= 5

fig = go.Figure(plot_point_cloud(point_cloud2))
fig.show()

dgm2 = ripser_parallel(point_cloud2, maxdim=1, return_generators=True)
print(dgm2['dgms'][1])
dgm_gtda = _postprocess_diagrams([dgm2["dgms"]], "ripser", (0,1), np.inf, True)[0]
fig = go.Figure(plot_diagram(dgm_gtda, homology_dimensions=(0,1)))
fig.show()

point_cloud2 = torch.tensor(point_cloud2, dtype=torch.float32)

number_of_iterations = 7500

point_clouds = []
losses3 = []

#initial point cloud: 2 lines with added noise

number_of_points = 64

newdata = np.zeros((number_of_points,2))
r1 = 0.1
for i in range(32):
  newdata[i][0] = random.uniform(-r1, r1)
  newdata[i][1] = float(i)*0.7 + random.uniform(-r1, r1)
  newdata[i+32][0] = random.uniform(-r1, r1) + 5. + float(i) * 0.2
  newdata[i+32][1] = float(i)*0.9 + random.uniform(-r1, r1)

point_cloud = torch.tensor(newdata, dtype=torch.float32, requires_grad=False)

# plot the initial PD:
dgm = ripser_parallel(newdata, maxdim=1, return_generators=True)
dgm_gtda = _postprocess_diagrams([dgm["dgms"]], "ripser", (0,1), np.inf, True)[0]
fig = go.Figure(plot_diagram(dgm_gtda, homology_dimensions=(0,1)))
fig.show()

point_cloud.requires_grad = True
point_clouds.append(np.copy(point_cloud.detach().numpy()))
optimizer = torch.optim.Adam([point_cloud], lr=0.1)

#plot initial point cloud:
fig = go.Figure(plot_point_cloud(point_clouds[-1]))
fig.show()

for i in range(number_of_iterations):
    loss, bdist = loss_topo0(point_cloud)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    if i % 50 == 0 or i==number_of_iterations - 1:  point_clouds.append(np.copy(point_cloud.detach().numpy()))

    if i == 0 or i == number_of_iterations-1:
        print(f"Iteration {i + 1}/{number_of_iterations}, Loss: {loss.item()}")
        fig = go.Figure(plot_point_cloud(point_clouds[-1]))
        fig.show()

        # get PD of latest point cloud and plot it:
        dgm = ripser_parallel(point_clouds[-1], maxdim=1, return_generators=True)
        dgm_gtda = _postprocess_diagrams([dgm["dgms"]], "ripser", (0, 1), np.inf, True)[0]
        fig = go.Figure(plot_diagram(dgm_gtda, homology_dimensions=(0,1)))
        fig.show()
    if i % 5 == 0 or i==number_of_iterations - 1:
      with torch.no_grad(): losses3.append(bdist)

xs = []
for i in range(len(losses3)): xs.append(i*5)
plt.plot(xs, losses3)
plt.xlabel("Iteration")
plt.ylim([0,3])
plt.ylabel("$L_1$")
plt.show()

generate_gif(point_clouds)
